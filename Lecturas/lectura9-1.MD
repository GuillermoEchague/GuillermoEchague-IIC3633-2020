# Lectura 13

## Multi-armed recommender system bandit ensembles

En ente trabajo se hace relaciÃ³n a como la configuraciÃ³n de los conjuntos obtienen mejores resultados que por si solos, pero solo evaluados en sistemas de dominio acotado. Lo interesante de este trabajo es el hecho de realizar pruebas en ambientes mÃ¡s reales que sean capaces de representar el trabajo cÃ­clico que realizan los sistemas.

Lo interesante que mencionan en este paper es el hecho de agregar valor al algoritmo de recomendaciÃ³n, que, aunque se han estudiado varios modelos, con buenos resultados, el desarrollo de un algoritmo hibrido tiene relevancia si es que agrega valor en la recomendaciÃ³n. 

El principal aporte de este trabajo es el anÃ¡lisis critico que existe en los trabajos presentados hasta el momento y la mejora que ellos proponen, que es en Ã¡mbito del testing del modelo. Ya que el proceso cÃ­clico brinda la oportunidad para que los conjuntos prueben, observen y aprendan sobre la efectividad de los sistemas combinados, y mejoren la configuraciÃ³n del conjunto progresivamente, lanzando la configuraciÃ³n del conjunto como una tarea de aprendizaje por refuerzo.

AcÃ¡ se adaptan dos algoritmos de bandidos bÃ¡sicos, muestreo de Thompson y Îµ-greedy, y se verifican que los enfoques resultantes son empÃ­ricamente mÃ¡s efectivos que las tÃ©cnicas de conjuntos alternativas que carecen de la perspectiva a largo plazo en experimentos basados en conjuntos de datos fuera de lÃ­nea.

Dentro de los trabajos relacionados, se menciona una aplicaciÃ³n que es las pruebas A / B, donde un mÃ©todo de bandidos decide automÃ¡ticamente entre varios algoritmos en funciÃ³n de su interpretaciÃ³n anterior.

La selecciÃ³n del algoritmo a recomendar en cada paso se basa en su desempeÃ±o en los ciclos anteriores en los que se ha seleccionado. Por ejemplo, en Îµ-greedy, se selecciona con probabilidad 1 âˆ’ Îµ, y con probabilidad Îµ, un algoritmo se selecciona de manera uniforme al azar independientemente de su efectividad histÃ³rica. Para el muestreo de Thompson, la parte posterior de la distribuciÃ³n de recompensa desconocida de cada brazo se modela como una distribuciÃ³n Beta (ğ›¼ğ‘, ğ›½ğ‘), donde ğ›¼ğ‘ y ğ›½ğ‘ son el nÃºmero de recomendaciones exitosas y no exitosas del algoritmo ğ‘, respectivamente. 

Para verificar el rendimiento de los conjuntos de recomendaciÃ³n de bandidos se ejecutan los algoritmos utilizando datos del conjunto de datos MovieLens 1M, que contiene 1.000.209 calificaciones de 6.040 usuarios a 3.706 pelÃ­culas. Binarizando las calificaciones mapeando los valores 1-3 a 0 y 4-5 a 1.

Dentro de los resultados obtenido de los algoritmos de filtrado colaborativo no pueden funcionar mejor que la recomendaciÃ³n por popularidad. Esto puede atribuirse a su vulnerabilidad a la escasez de datos inicial, mientras que la popularidad no personalizada obtiene una ventaja en las primeras iteraciones, cuando no hay suficientes datos para el filtrado colaborativo orientado a la explotaciÃ³n para producir recomendaciones personalizadas confiables.